{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive models using a feedforward neural network \n",
    "## PART 2: Applying the methods to health care time series\n",
    "\n",
    "In this notebook we will use a feedforward neural network to fit a single and ensemble linear and non-linear models to real time series data. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "1. Most of the work we will do is data manipulation: preprocessing data and making sure it is the right shape for the neural networks.\n",
    "\n",
    "2. The ensemble learning method can be computationally expensive.  We have included some pre-trained models that can be loaded from file if needed.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES** ðŸ“ˆ\n",
    "\n",
    "By the end of the notebook you will be able to\n",
    "\n",
    "âœ… Apply feedforward neural networks to real health care time series datasets.\n",
    "\n",
    "âœ… Understand how we pre-process time series data into tabular format suitable for Autoregressive networks.\n",
    "\n",
    "âœ… Gain practical experience of managing and exploiting the stochastic nature of neural network training.\n",
    "\n",
    "âœ… Forecast health time series using an ensemble of neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "**Here is our general plan to use with each model:**\n",
    "\n",
    "Before we do any model building let's think about a general approach to ensure we keep the analysis as simple as possible\n",
    "\n",
    "1. Conduct a temporal train-test split on the raw data into train and test (12 months).\n",
    "2. Scale the data ready for a neural network by fitting ONLY on train.\n",
    "3. Tabularise the training data ready to train the neural network model.\n",
    "4. Fit the data to the model\n",
    "5. Forecast by taking the **last window** of the training set and iterating forward 12 times.\n",
    "6. Calculate the root mean squared error of the forecast against the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided. We are again going to implement neural networks using `tensorflow` and '`keras`. You should be using at least `tensorflow` version `2.1.15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from typing import Tuple, Optional, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#  tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# forecast tools helps\n",
    "from forecast_tools.metrics import root_mean_squared_error\n",
    "from forecast_tools.plotting import plot_time_series\n",
    "\n",
    "# to scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting emergency admissions in England\n",
    "\n",
    "We will now use feedforward neural networks to predict the number of monthly emergency admissions in England. \n",
    "\n",
    "## Load the data and peek at the data\n",
    "\n",
    "**Task**:\n",
    "* Execute the code below to read the emergency admissions data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/health-data-science-OR/data/master' \\\n",
    "        + '/em_admits_ts.csv'\n",
    "em_admits = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "### Datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the the `month_year` column in `em_admits` holds a string an invalid date format e.g. 'Aug-10'.  Pandas cannot handle this as-is because '10' could refer to any century!  So let's do a bit of preprocessing to get it into a valid datetime format.\n",
    "\n",
    "\n",
    "*Optional Task:*\n",
    "* Take some time to understand the function `load_clean_admissions_dataset` that preprocesses the dates.  This is real health data and it is likely you will need to deal with formatting issues as experienced here.\n",
    "\n",
    "First we will format the string to something pandas can parse i.e. 'Aug 2010'.  Then we will call the `pd.to_datetime()` function to parse the string and return a `datetime`.  We will assign the result to our dataframe's index and set the freq to monthly start 'MS'\n",
    "\n",
    "> If you need help with `pandas` and **method chaining pipelines** like that implemented in `load_clean_admissions_dataset` the below then you can refresh your memory from my [online tutorial](https://www.pythonhealthdatascience.com/content/02_stat_prog/01_pandas/05_analysing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_admissions_dataset(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean emergency admissions data using pandas method chaining.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    url: str\n",
    "        URL or path to CSV file containing emergency admissions data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataframe with datetime index and 'MS' frequency\n",
    "    \"\"\"\n",
    "    em_admits = (\n",
    "        pd.read_csv(url)\n",
    "        .assign(\n",
    "            date=lambda df: pd.to_datetime(\n",
    "                df['month_year'].str[:3] + ' 20' + df['month_year'].str[-2:],\n",
    "                format='%b %Y'\n",
    "            )\n",
    "        )\n",
    "        .set_index('date')\n",
    "        .drop(columns=['month_year'])\n",
    "    )\n",
    "    \n",
    "    # Set frequency\n",
    "    em_admits.index.freq = 'MS'\n",
    "    \n",
    "    return em_admits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits = load_clean_admissions_dataset(url)\n",
    "em_admits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be forecasting the last 12 months of the series.  Let's take a look at the training data (being careful to exclude the last 12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_length = 12\n",
    "\n",
    "# forecast tools interactive plotting function (useful for looking at peaks/troughs in data)\n",
    "_ = plot_time_series(\n",
    "    training_data=em_admits[:-holdout_length],\n",
    "    y_axis_label=\"Emergency Admissions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is monthly data so a useful preprocessing step is to transform the data into a daily rate by dividing by the number of days in the month. When we plot this the troughs we saw in Feb each year disappear.\n",
    "\n",
    "**execute the code below which:**:\n",
    "* Calculates the average admissions per day series\n",
    "* Plots the training data (holding back 12 months for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_rate = em_admits['em_admits'] / em_admits.index.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_time_series(\n",
    "    training_data=pd.DataFrame(admit_rate)[:-holdout_length],\n",
    "    y_axis_label=\"Emergency Admissions Rate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Train-Test Split\n",
    "\n",
    "We will formally split the data so that we hold out 12 months for comparison of forecasting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_length = 12\n",
    "\n",
    "# training data\n",
    "admit_rate_train = admit_rate[:len(admit_rate)-holdout_length]\n",
    "\n",
    "# test data (don't peek!)\n",
    "admit_rate_test = admit_rate[-holdout_length:]\n",
    "\n",
    "print(admit_rate_train.shape)\n",
    "print(admit_rate_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1**: Process the time series to format suitable for supervised learning.\n",
    "\n",
    "### Scaling the features and target to be between -1 and 1\n",
    "In many machine learning applications data are scaled to be between 0 and 1. For neural network forecasting, *Ord, Fildes and Kourentzes (2017)* recommend scaling to be between -1 and 1.  This is what we will do here.  To do the scaling we will use\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.MinMaxScaler\n",
    "```\n",
    "\n",
    "> Execute the code below to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 2D array (n_samples, 1) as required by MinMaxScaler\n",
    "train_reshaped = admit_rate_train.values.reshape(-1, 1)\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit the scaler ONLY on training data\n",
    "scaler.fit(train_reshaped)\n",
    "\n",
    "# Transform the training data\n",
    "train_scaled = scaler.transform(train_reshaped).reshape(-1, )\n",
    "\n",
    "print(f\"Scaled Train Min: {train_scaled.min()}\")\n",
    "print(f\"Scaled Train Max: {train_scaled.max()}\")\n",
    "print(f\"Scaled Train Shape: {train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabularise the training data\n",
    "\n",
    "The function `sliding_window` has been provided below for you to create your training data.\n",
    "\n",
    "**Task**:\n",
    "* Using a sliding window approach convert the time series into a tabular format. \n",
    " * Use a window size of 12 and assume you are predicting a scalar value of y (1-step ahead).\n",
    "* Conduct a train test split holding back 12 windows as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    for i in range(0, len(train) - window_size - horizon + 1):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 12\n",
    "X_train, y_train = sliding_window(train_scaled, window_size=WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convince ourselves that our data is in tabular format suitable for regression.  The code below converts the windows to a `DataFrame` so we can easily visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_windows_to_dataframe(X_data, y_data) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Utility function for converting the window format data into \n",
    "    a pandas dataframe\n",
    "    \"\"\"\n",
    "    tabular_form = pd.concat([pd.DataFrame(X_data), pd.DataFrame(y_data)], axis=1)\n",
    "    columns = [f'lag_{i}' for i in range(len(X_train[0]), 0, -1)]\n",
    "    columns.append('y_t')\n",
    "    tabular_form.columns = columns\n",
    "    return tabular_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe and round 2 dp.\n",
    "convert_windows_to_dataframe(X_train, y_train).head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a quick validation we should be able to see that the last y_t is 1.0 \n",
    "# the maximum value in the training data (due to trend)\n",
    "convert_windows_to_dataframe(X_train, y_train).tail(2).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2**: A Linear regression model benchmark\n",
    "\n",
    "The first model we will try is the linear model. Its will serve as our neural network baseline.  (In practice we would also check this is better than a naive method such as seasonal naive).\n",
    "\n",
    "## Exercise 2a Train the model\n",
    "\n",
    "**Task:**\n",
    "* Using `Keras`, construct a neural network that mimics a simple linear regression model (see previous notebook).  \n",
    "* Optional: To get comparable results, set the tensorflow random number seed to 42\n",
    "* Train the model for 100 epochs.\n",
    "* Optionally you can use an early stopping callback with patience set to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_model(ws, lr=0.01, metrics=None):\n",
    "    '''\n",
    "    Sequential Keras model that minics\n",
    "    AR linear model. \n",
    "    '''\n",
    "    if metrics is None:\n",
    "        metrics = ['mae', 'mse']\n",
    "    \n",
    "    model = Sequential([Dense(1, input_shape=(ws,))])\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(learning_rate=lr),\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tensorflow random seed for repeatability\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# call the linear model function create earlier.\n",
    "model_lm = get_linear_model(ws=12, metrics=['mae'])\n",
    "\n",
    "# fit model silently (verbose=0)\n",
    "results = model_lm.fit(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    epochs=N_EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(results.history['loss'], label='loss')\n",
    "_ = plt.plot(results.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Plot the fitted values\n",
    "\n",
    "To see the in-sample fitted values (predicting the training data) we first need to back transform the predictions using the `scaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the predictions\n",
    "in_sample_preds = model_lm.predict(X_train)\n",
    "\n",
    "# back transform to original units\n",
    "back_transformed_preds = scaler.inverse_transform(in_sample_preds)\n",
    "\n",
    "_ = plt.plot(scaler.inverse_transform(y_train.reshape(-1, 1)), label='ground truth')\n",
    "_ = plt.plot(back_transformed_preds, label='NN fitted')\n",
    "\n",
    "_ = plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2b. Generate and evaluate a multi-step forecast\n",
    "\n",
    "**Task:**\n",
    "* Using the iterative method produce a 12 step forecast. Save the predictions in a variable called `y_preds_lm`\n",
    "* Calculate the RMSE\n",
    "* Optional: Plot the results -> predictions versus test.\n",
    "\n",
    "**Hints:**\n",
    "* A function `autoregressive_iterative_forecast` is provided below.  (you could use this function or write your own if you prefer!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        y_pred = model.predict(current_X.reshape(1, -1), verbose=0)[0,0]\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        current_X = np.roll(current_X, shift=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next 12 months and plot\n",
    "H = 12\n",
    "WINDOW_SIZE = 12 # this is the number of lags included in the model\n",
    "\n",
    "# data used for prediction\n",
    "# we need to include the last 12 months of the standardised data\n",
    "model_input_data = train_scaled[-WINDOW_SIZE:]\n",
    "\n",
    "# generate the predictions using the iterative forecast\n",
    "y_preds_lm = autoregressive_iterative_forecast(model_lm, model_input_data, h=H)\n",
    "\n",
    "# reshape the predictions and back transform\n",
    "y_preds_lm = scaler.inverse_transform(y_preds_lm.reshape(-1, 1))\n",
    "\n",
    "# plot.\n",
    "_ = plot_time_series(\n",
    "    training_data = pd.DataFrame(admit_rate_train),\n",
    "    test_data = pd.DataFrame(admit_rate_test),\n",
    "    forecast = pd.DataFrame(y_preds_lm, index=admit_rate_test.index),\n",
    "    y_axis_label = \"EM Admit Rate (Patient/day)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lm = root_mean_squared_error(y_preds_lm, admit_rate_test)\n",
    "print(f\"RMSE(Linear Model) = {rmse_lm:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 3:** Training a non-linear deep network\n",
    "\n",
    "Now that you have got the basic structure and mechanics of the code you need for forecasting let's build a more complex model and compare the RMSE on the validation set to your simple linear model.\n",
    "\n",
    "**Task:** \n",
    "* Create a new neural network model with 2 hidden layers\n",
    "* Try different numbers of neurons for layer 1 and 2 respectively\n",
    "* Use a ReLU activation function.\n",
    "* Use the Adam optimiser with a learning rate of 0.01\n",
    "* Predict the next 12 months ahead\n",
    "* Calculate the RMSE\n",
    "\n",
    "**Hints:**\n",
    "* Feel free to experiment with the number of hidden layers, neurons and learning rate.\n",
    "* Perhaps try a dropout layer(s) if you feel your model is overfitting.\n",
    "* Set a tensorflow random seed if you want to be able to reproduce your results e.g. 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_model(\n",
    "    ws, \n",
    "    n_neurons_l1=5, \n",
    "    n_neurons_l2=5,\n",
    "    include_layer_two=False, \n",
    "    include_drop_out=False,\n",
    "    drop_out_rate=0.2, \n",
    "    lr=0.001, \n",
    "    metrics=None\n",
    "):\n",
    "    '''\n",
    "    A function to build/compile the network based on a number of user\n",
    "    set parameters\n",
    "    '''\n",
    "    if metrics is None:\n",
    "        metrics = ['mse']\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(ws,)))\n",
    "    model.add(Dense(n_neurons_l1, activation='relu'))\n",
    "    if include_layer_two:\n",
    "        if include_drop_out:\n",
    "            model.add(Dropout(drop_out_rate))\n",
    "        model.add(Dense(n_neurons_l2, activation='relu'))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(learning_rate=lr),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tensorflow random seed\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "N_EPOCHS = 200\n",
    "\n",
    "# Neural network model\n",
    "# For simplicity we will ignore overfitting in this example\n",
    "mlp = get_network_model(\n",
    "    ws=12, \n",
    "    n_neurons_l1=5, \n",
    "    include_layer_two=True, \n",
    "    n_neurons_l2=5,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# fit model silently\n",
    "results_mlp = mlp.fit(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    epochs=N_EPOCHS,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the predictions\n",
    "in_sample_preds = mlp.predict(X_train)\n",
    "\n",
    "# back transform to original units\n",
    "back_transformed_preds = scaler.inverse_transform(in_sample_preds)\n",
    "\n",
    "_ = plt.plot(scaler.inverse_transform(y_train.reshape(-1, 1)), label='ground truth')\n",
    "_ = plt.plot(back_transformed_preds, label='NN fitted')\n",
    "\n",
    "_ = plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next 12 months and plot\n",
    "H = 12\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "# data used for prediction\n",
    "# we need to include the last 12 months of the standardised data\n",
    "model_input_data = train_scaled[-WINDOW_SIZE:]\n",
    "\n",
    "# generate the predictions using the iterative forecast\n",
    "y_preds_mlp = autoregressive_iterative_forecast(mlp, model_input_data, h=H)\n",
    "\n",
    "# reshape the predictions and back transform\n",
    "y_preds_mlp = scaler.inverse_transform(y_preds_mlp.reshape(-1, 1))\n",
    "\n",
    "# plot the predictions\n",
    "_ = plot_time_series(\n",
    "    training_data = pd.DataFrame(admit_rate_train),\n",
    "    test_data = pd.DataFrame(admit_rate_test),\n",
    "    forecast = pd.DataFrame(y_preds_mlp, index=admit_rate_test.index),\n",
    "    y_axis_label = \"EM Admit Rate (Patient/day)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate forecast error of the non-linear model\n",
    "rmse_mlp = root_mean_squared_error(y_preds_mlp, admit_rate_test)\n",
    "\n",
    "# a reminder of the linear model test error\n",
    "print(f\"RMSE(Linear Model) = {rmse_lm:.1f}\")\n",
    "print(f\"RMSE(Neural Network) = {rmse_mlp:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Ensemble Learning\n",
    "\n",
    "In all of the examples above we have been setting a random seed for tensorflow.  This 'suggests' that if we used a different randon number seed we would get a slightly different result (this is due to both random initialisation of weights/biases and stochastic gradient descent). Neural networks are extremely flexible and have many parameters. This leads to one of the key challenges with neural networks - overfitting.  There are multiple ways to deal with overfitting.  In forecasting a common approach is to use an **ensemble** of models.  \n",
    "\n",
    "In an ensemble we train multiple models. \n",
    "\n",
    "### Training an ensemble\n",
    "\n",
    "We will train an ensemble of neural networks that mimic a linear model.  \n",
    "\n",
    "The code below has been provided for you to work through.\n",
    "\n",
    "* We set some parameters e.g. number of models in an the ensemble: 20 to 30 should be plenty.\n",
    "* We use a python loop to create and train each model and store the model in a python list.\n",
    "    * Optionally we can save the models to file and load pre-trained versions at a later date.\n",
    "* To predict we the need to loop through the collection of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_ensemble(n_models):\n",
    "    '''\n",
    "    Load the pre-trained ensemble models (only use if they exist!)\n",
    "    '''\n",
    "    models = []\n",
    "    url = './input'\n",
    "    for n in range(n_models):\n",
    "        model_n = tf.keras.models.load_model(f'{url}/ensemble_model_{n}.keras')\n",
    "        models.append(model_n)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to train the models.\n",
    "\n",
    "# ################ Parameters for the ensemble #################################\n",
    "# set random seed so that ensemble can be repeated.\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# number of models to create...\n",
    "N_MODELS = 20\n",
    "\n",
    "# max no. of epochs for training of each model.\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# no. of autoregressive lags\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "# early stopping reguluarization\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# I've pretrained some models you can load them from file if wanted.\n",
    "LOAD_FROM_FILE = False\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    #it will take a few seconds to load.\n",
    "    models = load_pretrained_ensemble(N_MODELS)\n",
    "else:\n",
    "    print(\"# Training model: \", end=\"\")\n",
    "    models = []\n",
    "    for n in range(N_MODELS):\n",
    "        print(f\"{n+1},\", end=\"\")\n",
    "        # linear network\n",
    "        model_n = get_linear_model(WINDOW_SIZE)\n",
    "\n",
    "        # fit model silently (verbose=0)\n",
    "        history = model_n.fit(\n",
    "            x=X_train, \n",
    "            y=y_train, \n",
    "            epochs=N_EPOCHS,\n",
    "            verbose=0, \n",
    "            callbacks=[es], \n",
    "            validation_split=0.1\n",
    "        )\n",
    "\n",
    "        # this will overwrite pre-trained models.\n",
    "        model_n.save(f'input/ensemble_model_{n}.keras')\n",
    "        models.append(model_n)\n",
    "\n",
    "print(\"Training of ensemble complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions in an ensemble\n",
    "\n",
    "In an ensemble, we predict in a loop. In python this is straightfoward as we simply loop through the models we have trained and call `autoregressive_iterative_forecast`. We will store the predictions of each forecast in a python `list`  called `e_preds`\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "In an ensemble we end up with a distribution of forecasts!  For point forecasts we could then take the median of the forecasts.  We can also get a measure of variability in the forecasts by calculating the quantiles. \n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_forecast(trained_models, input_data, horizon, scaler, verbose=True) -> np.array:\n",
    "    \"\"\"\n",
    "    Iteratively forecast the time series using the ensemble models.  \n",
    "    Returns back transformed forecasts in a (horizon, len(train_models) array.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_models: list\n",
    "        Trained neural network models\n",
    "\n",
    "    input_data: np.array\n",
    "        Initial data to start to iterative forecast (e.g. last 12 lags)\n",
    "\n",
    "    horizon: int\n",
    "        Forecast horizon e.g. 6 or 12\n",
    "\n",
    "    scaler: MinMaxScaler\n",
    "        The scaler used to back transform the forecast to original units\n",
    "\n",
    "    verbose: bool, optional (default = True)\n",
    "        Print out progress or ensemble forecast.\n",
    "    \"\"\"\n",
    "    # stores ensemble predictions...\n",
    "    e_preds = []\n",
    "    \n",
    "    # loop through all models in the ensemble\n",
    "    if verbose:\n",
    "        print(\"Predicting using ensemble. Please wait.\")\n",
    "    for model in trained_models:\n",
    "        # iterative forecast for model_i\n",
    "        y_preds = autoregressive_iterative_forecast(model, input_data, h=horizon)\n",
    "        e_preds.append(y_preds)\n",
    "    \n",
    "    e_preds = np.array(e_preds)\n",
    "\n",
    "    # back transform\n",
    "    e_preds_tran = scaler.inverse_transform(e_preds).T\n",
    "\n",
    "    if verbose:\n",
    "        print(\"All predictions complete\")\n",
    "    return e_preds_tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the forecasts\n",
    "# this code will take a few seconds to execute\n",
    "H = 12\n",
    "WINDOW_SIZE = 12 # no. lags in the model\n",
    "\n",
    "# data used for prediction\n",
    "# we need to include the last 12 months of the standardised data\n",
    "model_input_data = train_scaled[-WINDOW_SIZE:]\n",
    "\n",
    "# get ensemble of forecasts this will also back transform the forecasts\n",
    "e_preds = ensemble_forecast(models, model_input_data, H, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 models all provided a forecast with horizon = 12 months\n",
    "e_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median of forecasts\n",
    "y_preds_mdn = np.percentile(e_preds.T, 50, axis=0)\n",
    "\n",
    "# 2.5 and 97.5 percentiles of distribution\n",
    "y_preds_2_5 = np.percentile(e_preds.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds.T, 97.5, axis=0)\n",
    "\n",
    "# effectively this reduces the data to one forecast model\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ensemble_forecasts(\n",
    "    y_test: ArrayLike,\n",
    "    ensemble_forecasts: ArrayLike,\n",
    "    y_median: ArrayLike,\n",
    "    y_lower: ArrayLike,\n",
    "    y_upper: ArrayLike,\n",
    "    figsize: Tuple[int, int] = (12, 4),\n",
    "    title_suffix: str = \"\",\n",
    "    legend_position: str = \"upper left\"\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots individual ensemble traces and the aggregate prediction intervals against \n",
    "    test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : ArrayLike\n",
    "        The ground truth test data (1D array).\n",
    "    ensemble_forecasts : ArrayLike\n",
    "        Raw forecasts from all models. Shape should be (n_timesteps, n_models).\n",
    "    y_median : ArrayLike\n",
    "        The median forecast (1D array).\n",
    "    y_lower : ArrayLike\n",
    "        The lower bound of the prediction interval (e.g., 2.5 percentile).\n",
    "    y_upper : ArrayLike\n",
    "        The upper bound of the prediction interval (e.g., 97.5 percentile).\n",
    "    figsize : Tuple[int, int], optional\n",
    "        Width and height of the figure in inches. Default is (12, 4).\n",
    "    title_suffix : str, optional\n",
    "        String to append to the subplot titles (e.g., \" - ARIMA\"). Default is empty.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The generated figure object.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistent indexing\n",
    "    y_test = np.array(y_test).flatten()\n",
    "    y_median = np.array(y_median).flatten()\n",
    "    y_lower = np.array(y_lower).flatten()\n",
    "    y_upper = np.array(y_upper).flatten()\n",
    "    ensemble_forecasts = np.array(ensemble_forecasts)\n",
    "    \n",
    "    n_models = ensemble_forecasts.shape[1]\n",
    "    time_steps = np.arange(len(y_test))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, sharey=True, figsize=figsize)\n",
    "\n",
    "    # --- Plot 1: Spaghetti Plot (Individual Traces) ---\n",
    "    # Plot ensemble members with high transparency to show density\n",
    "    ax[0].plot(\n",
    "        ensemble_forecasts, \n",
    "        color='gray', \n",
    "        alpha=0.2, \n",
    "        linewidth=0.8,\n",
    "        label='_nolegend_' # Hide from legend to prevent 100 entries\n",
    "    )\n",
    "    \n",
    "    # Add a dummy line just for the legend entry\n",
    "    ax[0].plot([], [], color='gray', alpha=0.5, label='Ensemble members')\n",
    "    \n",
    "    # Plot Median and Test\n",
    "    ax[0].plot(y_median, label='Median', linestyle='-', color='black', linewidth=1.5)\n",
    "    ax[0].plot(y_test, label='Test Actual', linestyle='--', color='red', linewidth=1.5)\n",
    "    \n",
    "    ax[0].set_title(f'Point forecasts: {n_models} models{title_suffix}')\n",
    "    ax[0].legend(loc=legend_position)\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "    ax[0].set_xlabel(\"Forecast Horizon\")\n",
    "    ax[0].set_ylabel(\"Admit Rate\")\n",
    "\n",
    "    # --- Plot 2: Prediction Intervals (Shaded Region) ---\n",
    "    # Plot Test and Median\n",
    "    ax[1].plot(y_test, label='Test Actual', linestyle='--', color='red', linewidth=1.5)\n",
    "    ax[1].plot(y_median, label='Median', linestyle='-', color='black', linewidth=1.5)\n",
    "    \n",
    "    # Plot Shaded Interval\n",
    "    ax[1].fill_between(\n",
    "        time_steps, \n",
    "        y_lower, \n",
    "        y_upper, \n",
    "        color='gray', \n",
    "        alpha=0.3, \n",
    "        label='95% Prediction Interval'\n",
    "    )\n",
    "    \n",
    "    # Optional: Keep the lines for sharp boundaries if preferred\n",
    "    ax[1].plot(y_lower, linestyle=':', color='gray', alpha=0.5, linewidth=1)\n",
    "    ax[1].plot(y_upper, linestyle=':', color='gray', alpha=0.5, linewidth=1)\n",
    "\n",
    "    ax[1].set_title(f'Uncertainty (Middle 95%){title_suffix}')\n",
    "    ax[1].legend(loc=legend_position)\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "    ax[1].set_xlabel(\"Forecast Horizon\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot side by side charts of ensemble predictions and internvals\n",
    "fig = plot_ensemble_forecasts(\n",
    "    y_test=admit_rate_test, \n",
    "    ensemble_forecasts=e_preds,\n",
    "    y_median=y_preds_mdn, \n",
    "    y_lower=y_preds_2_5, \n",
    "    y_upper=y_preds_97_5,\n",
    "    legend_position=\"lower left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate forecast error of the ensemble of linear models\n",
    "rmse_el = root_mean_squared_error(y_preds_mdn, admit_rate_test)\n",
    "\n",
    "# a reminder of the linear model test error\n",
    "print(f\"RMSE(Linear Model) = {rmse_lm:.1f}\")\n",
    "print(f\"RMSE(Neural Network) = {rmse_mlp:.1f}\")\n",
    "print(f\"RMSE(Linear Ensemble) = {rmse_el:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the variability in the predictions across models.\n",
    "\n",
    "individual_rmses = []\n",
    "for i in range(e_preds.shape[1]):  # Loop through each model\n",
    "    model_forecast = e_preds[:, i]  # Get this model's forecast\n",
    "    model_rmse = root_mean_squared_error(model_forecast, admit_rate_test)\n",
    "    individual_rmses.append(model_rmse)\n",
    "\n",
    "# Convert to array and find percentiles\n",
    "individual_rmses = np.array(individual_rmses)\n",
    "rmse_2_5 = np.percentile(individual_rmses, 2.5)\n",
    "rmse_97_5 = np.percentile(individual_rmses, 97.5)\n",
    "\n",
    "print(f'95% of models have RMSE between: {rmse_2_5:.1f} - {rmse_97_5:.1f}')\n",
    "print(f\"RMSE(Linear Model) = {rmse_lm:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is the ensemble approach useful?  What does it tell us about our original linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Create an ensemble of non-linear models.\n",
    "\n",
    "Is the two layer model more accurate than the simple linear regression model and its ensemble counterpart?\n",
    "\n",
    "**Task:** \n",
    "\n",
    "* Create an ensemble of 20 models.\n",
    "* Each model should be based on your solution to exercise 2 (e.g. a neural network with 2 hidden layers)\n",
    "* Optional: save your models to file. (recommended)\n",
    "* Forecast the next 12 periods.\n",
    "* Calculate the RMSE of the forecast.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* You have **all of the code** you need to complete this task!\n",
    "* Remember to back transform your forecasts\n",
    "* Use the median of the ensemble.\n",
    "* Look carefully at the previous ensemble example.\n",
    "\n",
    "**Questions**\n",
    "* Which out of the simple linear, multi-layer and ensemble models do you think is best in this instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tensorflow random seed for repeatability\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "N_MODELS = 20\n",
    "N_EPOCHS = 100\n",
    "H = 12\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "BATCH_SIZE = 32\n",
    "output_directory = \"./output\"\n",
    "\n",
    "# make if not present.\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "\n",
    "models = []\n",
    "print(\"# Training model: \", end=\"\")\n",
    "for n in range(N_MODELS):\n",
    "    print(f\"{n+1},\", end=\"\")\n",
    "    # multi-layer model\n",
    "    model_n = get_network_model(ws=12, \n",
    "                                n_neurons_l1=5,\n",
    "                                include_layer_two=True,\n",
    "                                n_neurons_l2=5,\n",
    "                                lr=0.01)\n",
    "\n",
    "    # fit model silently\n",
    "    history = model_n.fit(x=X_train, \n",
    "                          y=y_train, \n",
    "                          epochs=N_EPOCHS,\n",
    "                          verbose=0)\n",
    "\n",
    "\n",
    "    # this will overwrite pre-trained models.\n",
    "    model_n.save(f'output/mlp_ensemble_{n}.keras')\n",
    "    models.append(model_n)\n",
    "\n",
    "print(\"Training of ensemble complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the forecasts\n",
    "# this code will take a few seconds to execute\n",
    "H = 12\n",
    "WINDOW_SIZE = 12 # no. lags in the model\n",
    "\n",
    "# data used for prediction\n",
    "# we need to include the last 12 months of the standardised data\n",
    "model_input_data = train_scaled[-WINDOW_SIZE:]\n",
    "\n",
    "# get ensemble of forecasts this will also back transform the forecasts\n",
    "e_preds_mlp = ensemble_forecast(models, model_input_data, H, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_mdn = np.percentile(e_preds_mlp.T, 50, axis=0)\n",
    "y_preds_2_5 = np.percentile(e_preds_mlp.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds_mlp.T, 97.5, axis=0)\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the median of the predictions\n",
    "_ = plot_time_series(\n",
    "    training_data = pd.DataFrame(admit_rate_train),\n",
    "    test_data = pd.DataFrame(admit_rate_test),\n",
    "    forecast = pd.DataFrame(y_preds_mdn, index=admit_rate_test.index),\n",
    "    y_axis_label = \"EM Admit Rate (Patient/day)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot side by side charts of ensemble predictions and internals\n",
    "fig = plot_ensemble_forecasts(\n",
    "    y_test=admit_rate_test, \n",
    "    ensemble_forecasts=e_preds_mlp,\n",
    "    y_median=y_preds_mdn, \n",
    "    y_lower=y_preds_2_5, \n",
    "    y_upper=y_preds_97_5,\n",
    "    legend_position=\"upper left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate forecast error of the ensemble of linear models\n",
    "rmse_emlp = root_mean_squared_error(y_preds_mdn, admit_rate_test)\n",
    "\n",
    "# a reminder of the linear model test error\n",
    "print(f\"RMSE(Linear Model) = {rmse_lm:.1f}\")\n",
    "print(f\"RMSE(Neural Network) = {rmse_mlp:.1f}\")\n",
    "print(f\"RMSE(Linear Ensemble) = {rmse_el:.1f}\")\n",
    "print(f\"RMSE(Non-Linear Ensemble) = {rmse_emlp:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the variability in the predictions across models.\n",
    "\n",
    "individual_rmses = []\n",
    "for i in range(e_preds_mlp.shape[1]):  # Loop through each model\n",
    "    model_forecast = e_preds_mlp[:, i]  # Get this model's forecast\n",
    "    model_rmse = root_mean_squared_error(model_forecast, admit_rate_test)\n",
    "    individual_rmses.append(model_rmse)\n",
    "\n",
    "# Convert to array and find percentiles\n",
    "individual_rmses = np.array(individual_rmses)\n",
    "rmse_2_5 = np.percentile(individual_rmses, 2.5)\n",
    "rmse_97_5 = np.percentile(individual_rmses, 97.5)\n",
    "\n",
    "print(f'95% of models have RMSE between: {rmse_2_5:.1f} - {rmse_97_5:.1f}')\n",
    "print(f\"RMSE(Neural Network) = {rmse_mlp:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extra exercise for you to think about.\n",
    "* How would you use a ensemble method with a model that predicts a vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
